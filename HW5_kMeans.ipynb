{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlwaysEden/Machine-Learning-HW/blob/main/HW5_kMeans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount your drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "JMG2U5hkd-a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code\n",
        "\n",
        "1.   Load the MNIST dataset as in Homework 1.\n",
        "Filter the dataset into a subset that contains only the '3' and '9' images.\n",
        "\n",
        "2.   Implement and run the kMeans algorithm on this subset with k = 2, 3, 5, 10.\n",
        "\n",
        "3.   Reduce the dimensionality of the raw '3' and '9' images of the subset from 784 to eigenspaces 2, 5 and 10 dimensions (that is, you will have 3 new datasets). Implement and run the kMeans algorithm on these new subsets with k = 2, 3, 5, 10.\n",
        "\n",
        "In total, you'd run your kMeans 16 times."
      ],
      "metadata": {
        "id": "ZJJhenmlznr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import six.moves.cPickle as pickle\n",
        "import gzip\n",
        "import os\n",
        "import numpy as np\n",
        "import urllib.request"
      ],
      "metadata": {
        "id": "C_I_CsJhN01b"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(dataset):\n",
        "\n",
        "    if not os.path.isfile(dataset):\n",
        "        origin = (\n",
        "            'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
        "        )\n",
        "        print('Downloading data from %s' % origin)\n",
        "        urllib.request.urlretrieve(origin, dataset)\n",
        "\n",
        "    print('Loading data...')\n",
        "\n",
        "    with gzip.open(dataset, 'rb') as f:\n",
        "        try:\n",
        "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
        "        except:\n",
        "            train_set, valid_set, test_set = pickle.load(f)\n",
        "\n",
        "    all_data_x = np.concatenate([train_set[0], valid_set[0], test_set[0]])\n",
        "    all_data_y = np.concatenate([train_set[1], valid_set[1], test_set[1]])\n",
        "\n",
        "    print('... data has been loaded!')\n",
        "    return all_data_x, all_data_y"
      ],
      "metadata": {
        "id": "9nhdsdIbhgCB"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_k(number_of_k, images):\n",
        "  k = [None] * number_of_k\n",
        "  for i in range(number_of_k):\n",
        "    k_index = np.random.randint(0, len(images))\n",
        "    k[i] = images[k_index].copy()\n",
        "  return k\n",
        "\n",
        "def euclidean_distance(x1, x2):\n",
        "    distance = np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "    return distance\n",
        "\n",
        "def clustering(k, cluster, images):\n",
        "  for i in range(len(images)):\n",
        "    distances = [None] * len(k)\n",
        "    for j in range(len(k)):\n",
        "      distances[j] = euclidean_distance(images[i], k[j])\n",
        "    cluster[i] = np.argmin(distances)\n",
        "\n",
        "def kMeans(num_k, images):\n",
        "  cluster = [None] * len(images)\n",
        "  k = create_k(num_k, images) # Assign k randomly\n",
        "  tolerance = 0.0001 # Check if the mean change\n",
        "  stop_signal = 0\n",
        "  while stop_signal < num_k:\n",
        "    stop_signal = 0\n",
        "    clustering(k, cluster, images)\n",
        "    for i in range(num_k):\n",
        "      cluster_index = np.where(np.array(cluster)==i)[0]\n",
        "      mean = np.mean(images[cluster_index], axis=0)\n",
        "      if np.linalg.norm(mean - k[i]) < tolerance:\n",
        "        stop_signal += 1\n",
        "      k[i] = mean.copy()\n",
        "\n",
        "  return cluster.copy()\n",
        "\n",
        "def showImages(k, cluster, images):\n",
        "  index = [None] * k\n",
        "  for i in range(k):\n",
        "    index[i] = np.where(np.array(cluster)==i)[0]\n",
        "\n",
        "  for x in range(k):\n",
        "    for i in index[x]:\n",
        "      plt.imshow(images[i].reshape(28,28))\n",
        "      plt.show()\n",
        "      if i > 50:\n",
        "        print(\"***********************************\")\n",
        "        break\n",
        "\n",
        "def PCA(k, images):\n",
        "  mean = np.mean(images, axis=0)\n",
        "  data_center = images - mean\n",
        "  cov_matrix = np.cov(data_center, rowvar=False)\n",
        "\n",
        "  eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "  sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "  sorted_eigenvalues = eigenvalues[sorted_indices]\n",
        "  sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "  principal_components = sorted_eigenvectors[:, :k]\n",
        "\n",
        "  scaled_images = np.dot(data_center, principal_components)\n",
        "  return scaled_images"
      ],
      "metadata": {
        "id": "rSCeFQCTYwiV"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the data into train, validation and test sets\n",
        "data_x, data_y= load_data('mnist.pkl.gz')\n",
        "\n",
        "indices_3 = np.where(data_y == 3)[0]\n",
        "indices_9 = np.where(data_y == 9)[0]\n",
        "\n",
        "images_3 = data_x[indices_3]\n",
        "images_9 = data_x[indices_9]\n",
        "\n",
        "images = np.concatenate([images_3, images_9])\n",
        "np.random.shuffle(images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "682hpyjKNr9a",
        "outputId": "3900908e-586f-447d-eaf6-fd19cb855699"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "... data has been loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#########################################\n",
        "\n",
        "# k = 2, dim = 784\n",
        "cluster_2_784 = kMeans(2, images)\n",
        "\n",
        "# k = 3, dim = 784\n",
        "cluster_3_784 = kMeans(3, images)\n",
        "\n",
        "# k = 5, dim = 784\n",
        "cluster_5_784 = kMeans(5, images)\n",
        "\n",
        "# k = 10, dim = 784\n",
        "cluster_10_784 = kMeans(10, images)\n",
        "\n",
        "#########################################\n",
        "scaled_images = PCA(2, images)\n",
        "\n",
        "# k = 2, dim = 2\n",
        "cluster_2_2 = kMeans(2, scaled_images)\n",
        "\n",
        "# k = 3, dim = 2\n",
        "cluster_3_2 = kMeans(3, scaled_images)\n",
        "\n",
        "# k = 5, dim = 2\n",
        "cluster_5_2 = kMeans(5, scaled_images)\n",
        "\n",
        "# k = 10, dim = 2\n",
        "cluster_10_2 = kMeans(10, scaled_images)\n",
        "\n",
        "#########################################\n",
        "scaled_images = PCA(5, images)\n",
        "\n",
        "# k = 2, dim = 5\n",
        "cluster_2_5 = kMeans(2, scaled_images)\n",
        "\n",
        "# k = 3, dim = 5\n",
        "cluster_3_5 = kMeans(3, scaled_images)\n",
        "\n",
        "# k = 5, dim = 5\n",
        "cluster_5_5 = kMeans(5, scaled_images)\n",
        "\n",
        "# k = 10, dim = 5\n",
        "cluster_10_5 = kMeans(10, scaled_images)\n",
        "\n",
        "#########################################\n",
        "\n",
        "scaled_images = PCA(10, images)\n",
        "\n",
        "# k = 2, dim = 10\n",
        "cluster_2_10 = kMeans(2, scaled_images)\n",
        "\n",
        "# k = 3, dim = 10\n",
        "cluster_3_10 = kMeans(3, scaled_images)\n",
        "\n",
        "# k = 5, dim = 10\n",
        "cluster_5_10 = kMeans(5, scaled_images)\n",
        "\n",
        "# k = 10, dim = 10\n",
        "cluster_10_10 = kMeans(10, scaled_images)"
      ],
      "metadata": {
        "id": "qkRd6kqrhWht"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Roughly Checking Cluster\n",
        "#control k and cluster name(cluster_2_784, cluster_3_784, cluster_5_784, cluster_10_784)\n",
        "'''\n",
        "print(\"*****k=2, dim = 784******\")\n",
        "showImages(2, cluster_2_784, images)\n",
        "print(\"*****k=3, dim = 784******\")\n",
        "showImages(3, cluster_3_784, images)\n",
        "print(\"*****k=5, dim = 784******\")\n",
        "showImages(5, cluster_5_784, images)\n",
        "print(\"*****k=10, dim = 784******\")\n",
        "showImages(10, cluster_10_784, images)\n",
        "''"
      ],
      "metadata": {
        "id": "MC3M4rxJqHGw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Roughly Checking Cluster\n",
        "#control k and cluster name(cluster_2_2, cluster_3_2, cluster_5_2, cluster_10_2)\n",
        "'''\n",
        "print(\"*****k=2, dim = 2******\")\n",
        "showImages(2, cluster_2_2, images)\n",
        "print(\"*****k=3, dim = 2******\")\n",
        "showImages(3, cluster_3_2, images)\n",
        "print(\"*****k=5, dim = 2******\")\n",
        "showImages(5, cluster_5_2, images)\n",
        "print(\"*****k=10, dim = 2******\")\n",
        "showImages(10, cluster_10_2, images)\n",
        "'''"
      ],
      "metadata": {
        "id": "vgNRCAGQedqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Roughly Checking Cluster\n",
        "#control k and cluster name(cluster_2_5, cluster_3_5, cluster_5_5, cluster_10_5)\n",
        "'''\n",
        "print(\"*****k=2, dim = 2******\")\n",
        "showImages(2, cluster_2_5, images)\n",
        "print(\"*****k=3, dim = 2******\")\n",
        "showImages(3, cluster_3_5, images)\n",
        "print(\"*****k=5, dim = 2******\")\n",
        "showImages(5, cluster_5_5, images)\n",
        "print(\"*****k=10, dim = 2******\")\n",
        "showImages(10, cluster_10_5, images)\n",
        "'''"
      ],
      "metadata": {
        "id": "LKb0TnGNtBXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Roughly Checking Cluster\n",
        "#control k and cluster name(cluster_2_10, cluster_3_10, cluster_5_10, cluster_10_10)\n",
        "'''\n",
        "print(\"*****k=2, dim = 10******\")\n",
        "showImages(2, cluster_2_10, images)\n",
        "print(\"*****k=3, dim = 10******\")\n",
        "showImages(3, cluster_3_10, images)\n",
        "print(\"*****k=5, dim = 10******\")\n",
        "showImages(5, cluster_5_10, images)\n",
        "print(\"*****k=10, dim = 10******\")\n",
        "showImages(10, cluster_10_10, images)\n",
        "'''"
      ],
      "metadata": {
        "id": "dNLHbEzmz0G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT prompts\n",
        "\n",
        "Now solve the same task with ChatGPT with the following prompts:\n",
        "\n",
        "\n",
        "1.   Can you provide a code to filter the MNIST dataset so that we only have the 9 and 3 samples?\n",
        "2.   Can you compute the kmeans of this filtered dataset without using any specialized library (coding kmeans from scratch) with k=2,3,5,10?\n",
        "3.   Can you reduce the dimensionality of the filtered dataset to eigenspaces of dimension 2, 5 and 10 instead of 784?\n",
        "4.   Can you use the kmeans function from scratch and compute it for the new projected datasets using k=2,3,5 and 10?\n",
        "\n",
        "**Paste the answer provided and run the code below:**"
      ],
      "metadata": {
        "id": "f9G3Rux537Yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(dataset):\n",
        "\n",
        "    # Download the MNIST dataset if it is not present\n",
        "    if not os.path.isfile(dataset):\n",
        "        origin = (\n",
        "            'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
        "        )\n",
        "        print('Downloading data from %s' % origin)\n",
        "        urllib.request.urlretrieve(origin, dataset)\n",
        "\n",
        "    print('Loading data...')\n",
        "\n",
        "    # Load the dataset\n",
        "    with gzip.open(dataset, 'rb') as f:\n",
        "        try:\n",
        "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
        "        except:\n",
        "            train_set, valid_set, test_set = pickle.load(f)\n",
        "\n",
        "    all_data_x = np.concatenate([train_set[0], valid_set[0], test_set[0]])\n",
        "    all_data_y = np.concatenate([train_set[1], valid_set[1], test_set[1]])\n",
        "\n",
        "    print('... data has been loaded!')\n",
        "    return all_data_x, all_data_y\n",
        "\n",
        "def filter_mnist(X, y, digits=(3, 9)):\n",
        "    \"\"\"\n",
        "    Filters the MNIST dataset to include only the specified digits.\n",
        "\n",
        "    :param X: NumPy array or list of images (shape: [number_of_samples, height, width])\n",
        "    :param y: NumPy array or list of labels (shape: [number_of_samples])\n",
        "    :param digits: A tuple containing the digits to filter for (default: (3, 9))\n",
        "    :return: Filtered images and labels for the specified digits\n",
        "    \"\"\"\n",
        "    # Initialize lists to hold the filtered data\n",
        "    filtered_X = []\n",
        "    filtered_y = []\n",
        "\n",
        "    # Iterate through each label in y\n",
        "    for i in range(len(y)):\n",
        "        if y[i] in digits:\n",
        "            filtered_X.append(X[i])\n",
        "            filtered_y.append(y[i])\n",
        "\n",
        "    return np.array(filtered_X), np.array(filtered_y)\n",
        "\n",
        "\n",
        "data_x, data_y= load_data('mnist.pkl.gz')\n",
        "X_filtered, y_filtered = filter_mnist(data_x, data_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT-m7R6IAqSb",
        "outputId": "2dde2622-b942-476e-9497-68ab4699c53f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "... data has been loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def initialize_centroids(X, k):\n",
        "    \"\"\" Randomly initialize centroids. \"\"\"\n",
        "    random_indices = np.random.choice(X.shape[0], k, replace=False)\n",
        "    return X[random_indices]\n",
        "\n",
        "def compute_distances(X, centroids):\n",
        "    \"\"\" Compute the distance from each point to each centroid. \"\"\"\n",
        "    distances = np.zeros((X.shape[0], centroids.shape[0]))\n",
        "    for i in range(centroids.shape[0]):\n",
        "        distances[:, i] = np.linalg.norm(X - centroids[i], axis=1)\n",
        "    return distances\n",
        "\n",
        "def assign_clusters(X, centroids):\n",
        "    \"\"\" Assign each data point to the nearest centroid. \"\"\"\n",
        "    distances = compute_distances(X, centroids)\n",
        "    return np.argmin(distances, axis=1)\n",
        "\n",
        "def update_centroids(X, labels, k):\n",
        "    \"\"\" Update centroids based on the mean of assigned points. \"\"\"\n",
        "    new_centroids = np.zeros((k, X.shape[1]))\n",
        "    for i in range(k):\n",
        "        new_centroids[i] = X[labels == i].mean(axis=0)\n",
        "    return new_centroids\n",
        "\n",
        "def kmeans(X, k, max_iters=100):\n",
        "    \"\"\" Run the KMeans algorithm. \"\"\"\n",
        "    centroids = initialize_centroids(X, k)\n",
        "\n",
        "    for _ in range(max_iters):\n",
        "        labels = assign_clusters(X, centroids)\n",
        "        new_centroids = update_centroids(X, labels, k)\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.all(centroids == new_centroids):\n",
        "            break\n",
        "\n",
        "        centroids = new_centroids\n",
        "\n",
        "    return labels, centroids\n",
        "\n",
        "# Example usage\n",
        "k_values = [2, 3, 5, 10]\n",
        "\n",
        "# Assuming X_filtered is your filtered dataset of shape (num_samples, 28*28)\n",
        "# Reshape images to (num_samples, 28*28) for KMeans\n",
        "X_flattened = X_filtered.reshape(X_filtered.shape[0], -1)\n",
        "\n",
        "for k in k_values:\n",
        "    print(f\"\\nRunning KMeans with k={k}...\")\n",
        "    labels, centroids = kmeans(X_flattened, k)\n",
        "\n",
        "    # For demonstration, let's print out the centroids and their corresponding labels\n",
        "    print(f\"Centroids:\\n{centroids}\")\n",
        "    print(f\"Labels (first 10): {labels[:10]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p89EORlVCuoh",
        "outputId": "ea65baf8-a574-48da-b844-c0a332075ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running KMeans with k=2...\n",
            "Centroids:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Labels (first 10): [0 1 1 1 0 0 1 1 0 0]\n",
            "\n",
            "Running KMeans with k=3...\n",
            "Centroids:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Labels (first 10): [2 1 1 0 2 2 1 0 2 2]\n",
            "\n",
            "Running KMeans with k=5...\n",
            "Centroids:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Labels (first 10): [1 2 2 0 3 3 2 0 3 3]\n",
            "\n",
            "Running KMeans with k=10...\n",
            "Centroids:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Labels (first 10): [2 5 3 7 4 4 5 7 4 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_data(X):\n",
        "    \"\"\" Standardize the dataset by centering it (mean = 0). \"\"\"\n",
        "    mean = np.mean(X, axis=0)\n",
        "    X_centered = X - mean\n",
        "    return X_centered\n",
        "\n",
        "def compute_covariance_matrix(X):\n",
        "    \"\"\" Compute the covariance matrix of the dataset. \"\"\"\n",
        "    return np.cov(X, rowvar=False)\n",
        "\n",
        "def compute_eigenvalues_and_eigenvectors(cov_matrix):\n",
        "    \"\"\" Compute eigenvalues and eigenvectors of the covariance matrix. \"\"\"\n",
        "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)  # Use eigh for symmetric matrix\n",
        "    return eigenvalues, eigenvectors\n",
        "\n",
        "def pca(X, k):\n",
        "    \"\"\" Perform PCA and reduce the dataset to k dimensions. \"\"\"\n",
        "    X_centered = standardize_data(X)\n",
        "    cov_matrix = compute_covariance_matrix(X_centered)\n",
        "\n",
        "    eigenvalues, eigenvectors = compute_eigenvalues_and_eigenvectors(cov_matrix)\n",
        "\n",
        "    # Sort eigenvalues and corresponding eigenvectors\n",
        "    sorted_indices = np.argsort(eigenvalues)[::-1]  # Sort in descending order\n",
        "    sorted_eigenvalues = eigenvalues[sorted_indices]\n",
        "    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "    # Select the top k eigenvectors\n",
        "    top_k_eigenvectors = sorted_eigenvectors[:, :k]\n",
        "\n",
        "    # Project the data onto the new eigenspace\n",
        "    X_reduced = np.dot(X_centered, top_k_eigenvectors)\n",
        "\n",
        "    return X_reduced, sorted_eigenvalues[:k]\n",
        "\n",
        "# Example usage\n",
        "k_values = [2, 5, 10]\n",
        "\n",
        "# Assuming X_filtered is your filtered dataset of shape (num_samples, 28*28)\n",
        "# Reshape images to (num_samples, 28*28) for PCA\n",
        "X_flattened = X_filtered.reshape(X_filtered.shape[0], -1)\n",
        "\n",
        "for k in k_values:\n",
        "    print(f\"\\nReducing dimensionality to k={k}...\")\n",
        "    X_reduced, eigenvalues = pca(X_flattened, k)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Reduced shape: {X_reduced.shape}\")\n",
        "    print(f\"Eigenvalues (first {k}): {eigenvalues}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLZG3FwJDBpf",
        "outputId": "128d3601-718b-4e6b-c431-e884b11d234e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reducing dimensionality to k=2...\n",
            "Reduced shape: (14099, 2)\n",
            "Eigenvalues (first 2): [7.62428636 3.98411468]\n",
            "\n",
            "Reducing dimensionality to k=3...\n",
            "Reduced shape: (14099, 3)\n",
            "Eigenvalues (first 3): [7.62428636 3.98411468 2.91091582]\n",
            "\n",
            "Reducing dimensionality to k=5...\n",
            "Reduced shape: (14099, 5)\n",
            "Eigenvalues (first 5): [7.62428636 3.98411468 2.91091582 2.3414852  2.17304102]\n",
            "\n",
            "Reducing dimensionality to k=10...\n",
            "Reduced shape: (14099, 10)\n",
            "Eigenvalues (first 10): [7.62428636 3.98411468 2.91091582 2.3414852  2.17304102 1.75331286\n",
            " 1.26774586 1.11704296 1.09760913 1.0540397 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import gzip\n",
        "import pickle\n",
        "import urllib.request\n",
        "\n",
        "def load_data(dataset):\n",
        "    # Download the MNIST dataset if it is not present\n",
        "    if not os.path.isfile(dataset):\n",
        "        origin = (\n",
        "            'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
        "        )\n",
        "        print('Downloading data from %s' % origin)\n",
        "        urllib.request.urlretrieve(origin, dataset)\n",
        "\n",
        "    print('Loading data...')\n",
        "\n",
        "    # Load the dataset\n",
        "    with gzip.open(dataset, 'rb') as f:\n",
        "        try:\n",
        "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
        "        except:\n",
        "            train_set, valid_set, test_set = pickle.load(f)\n",
        "\n",
        "    all_data_x = np.concatenate([train_set[0], valid_set[0], test_set[0]])\n",
        "    all_data_y = np.concatenate([train_set[1], valid_set[1], test_set[1]])\n",
        "\n",
        "    print('... data has been loaded!')\n",
        "    return all_data_x, all_data_y\n",
        "\n",
        "def standardize_data(X):\n",
        "    \"\"\" Standardize the dataset by centering it (mean = 0). \"\"\"\n",
        "    mean = np.mean(X, axis=0)\n",
        "    X_centered = X - mean\n",
        "    return X_centered\n",
        "\n",
        "def compute_covariance_matrix(X):\n",
        "    \"\"\" Compute the covariance matrix of the dataset. \"\"\"\n",
        "    return np.cov(X, rowvar=False)\n",
        "\n",
        "def compute_eigenvalues_and_eigenvectors(cov_matrix):\n",
        "    \"\"\" Compute eigenvalues and eigenvectors of the covariance matrix. \"\"\"\n",
        "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)  # Use eigh for symmetric matrix\n",
        "    return eigenvalues, eigenvectors\n",
        "\n",
        "def pca(X, k):\n",
        "    \"\"\" Perform PCA and reduce the dataset to k dimensions. \"\"\"\n",
        "    X_centered = standardize_data(X)\n",
        "    cov_matrix = compute_covariance_matrix(X_centered)\n",
        "\n",
        "    eigenvalues, eigenvectors = compute_eigenvalues_and_eigenvectors(cov_matrix)\n",
        "\n",
        "    # Sort eigenvalues and corresponding eigenvectors\n",
        "    sorted_indices = np.argsort(eigenvalues)[::-1]  # Sort in descending order\n",
        "    sorted_eigenvalues = eigenvalues[sorted_indices]\n",
        "    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "    # Select the top k eigenvectors\n",
        "    top_k_eigenvectors = sorted_eigenvectors[:, :k]\n",
        "\n",
        "    # Project the data onto the new eigenspace\n",
        "    X_reduced = np.dot(X_centered, top_k_eigenvectors)\n",
        "\n",
        "    return X_reduced, sorted_eigenvalues[:k]\n",
        "\n",
        "def initialize_centroids(X, k):\n",
        "    \"\"\" Randomly initialize centroids. \"\"\"\n",
        "    random_indices = np.random.choice(X.shape[0], k, replace=False)\n",
        "    return X[random_indices]\n",
        "\n",
        "def compute_distances(X, centroids):\n",
        "    \"\"\" Compute the distance from each point to each centroid. \"\"\"\n",
        "    distances = np.zeros((X.shape[0], centroids.shape[0]))\n",
        "    for i in range(centroids.shape[0]):\n",
        "        distances[:, i] = np.linalg.norm(X - centroids[i], axis=1)\n",
        "    return distances\n",
        "\n",
        "def assign_clusters(X, centroids):\n",
        "    \"\"\" Assign each data point to the nearest centroid. \"\"\"\n",
        "    distances = compute_distances(X, centroids)\n",
        "    return np.argmin(distances, axis=1)\n",
        "\n",
        "def update_centroids(X, labels, k):\n",
        "    \"\"\" Update centroids based on the mean of assigned points. \"\"\"\n",
        "    new_centroids = np.zeros((k, X.shape[1]))\n",
        "    for i in range(k):\n",
        "        new_centroids[i] = X[labels == i].mean(axis=0)\n",
        "    return new_centroids\n",
        "\n",
        "def kmeans(X, k, max_iters=100):\n",
        "    \"\"\" Run the KMeans algorithm. \"\"\"\n",
        "    centroids = initialize_centroids(X, k)\n",
        "\n",
        "    for _ in range(max_iters):\n",
        "        labels = assign_clusters(X, centroids)\n",
        "        new_centroids = update_centroids(X, labels, k)\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.all(centroids == new_centroids):\n",
        "            break\n",
        "\n",
        "        centroids = new_centroids\n",
        "\n",
        "    return labels, centroids\n",
        "\n",
        "# Load the MNIST dataset\n",
        "data_file = 'mnist.pkl.gz'\n",
        "X, y = load_data(data_file)\n",
        "\n",
        "# Filter the dataset for only digits 3 and 9\n",
        "mask = np.isin(y, [3, 9])\n",
        "X_filtered = X[mask]\n",
        "y_filtered = y[mask]\n",
        "\n",
        "# Flatten the filtered dataset for PCA\n",
        "X_flattened = X_filtered.reshape(X_filtered.shape[0], -1)\n",
        "\n",
        "# Define the k values for KMeans\n",
        "k_values = [2, 3, 5, 10]\n",
        "dim_values = [0,2,5,10]\n",
        "for k in k_values:\n",
        "  for dim in dim_values:\n",
        "    if dim == 0:\n",
        "      labels, centroids = kmeans(X_flattened, k)\n",
        "      print(f\"Running KMeans on reduced dataset with k={k}, dim=784...\")\n",
        "      continue\n",
        "    print(f\"\\nReducing dimensionality to k={k} using PCA...\")\n",
        "    X_reduced, eigenvalues = pca(X_flattened, dim)\n",
        "    labels, centroids = kmeans(X_reduced, k)\n",
        "\n",
        "    print(f\"Running KMeans on reduced dataset with k={k}, dim={dim}...\")\n",
        "\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Centroids:\\n{centroids}\")\n",
        "    #print(f\"Labels (first 10): {labels[:10]}\")\n"
      ],
      "metadata": {
        "id": "l1xDJUMwDX-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report\n",
        "\n",
        "\n",
        "\n",
        "1.   What are some differences you notice between your code and ChatGPT's? (Name at least 3)\n",
        "2.   Ask ChatGPT how can you judge if a cluster is correctly computed. Paste the answer here. Do you agree with ChatGPT's statement? Why yes or why not?\n",
        "3.   Ask ChatGPT how can we know which k is the correct parameter for a kMeans algorithm. Paste the answer here. Do you agree with ChatGPT's statement? Why yes or why not?\n",
        "4.   Conclude with some thoughts and things you learned from this homework."
      ],
      "metadata": {
        "id": "94QY3yc67Pwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Answer</h1>\n",
        "1.\n",
        "\n",
        "- Clustering End condition: ChatGPT made the kMeans code iterate up to 100 times, but I made the kMeans code iterate until the error is less than 0.001. I think this is a trade-off of time and accuracy for more accurate clustering, so ChatGPT is always faster to compute.\n",
        "- Filter MNIST: ChatGPT splits the data into 3 and 9 images using np.isin(). It can help me find the data easily.\n",
        "- Code Structure: I made the code structure for readability. However, ChatGPT made the code structure for efficiency.\n",
        "2.\n",
        "\n",
        "I generally agree with these idea. First of all, Within-cluster distance and Between-cluster is a core concepts of inner and outter cluster. When theses concepts are considered together, we can get a better clustering. Second, Elbow method and Visualization are a good approach. Elbow method is a mathematic way to prove it and the concept is very clear and understandable. And visualization is very intuitive. When we use lower-dimensional data, visualization is useful for us to understand it in one glance. However, Cluster interpretability and Silhouette score are carefully considered. Cluster interpretability can be used easily when the data is labeled. But, If the data is labled, better approaches exist. And when the data is not labled, it is based on the domain knowledge. If so, I would rather use a different algorithm. In Silhouette score, we need to think of the reason why all cluster should be similar. In my opinion, if the data is many different, the cluster is also many different.\n",
        "\n",
        "3.\n",
        "\n",
        "The methods which decide if k is correct are very similar to the methods which decided if cluster is correctly computed. These methods consist of mathematical approaches, approaches in terms of data, Heuristic approaches, and etc. I generally agree with these ideas the same way I agree with the cluster problem.\n",
        "\n",
        "4.\n",
        "\n",
        "- Eventully, Algorithm. General algorithms in CS are the methods to solve some problem using data structure and logical thinking. kMeans is also algorithm and it's just data with dimensions, not formal data like a data structure.\n",
        "- Understanding Dimension. MNIST image data is [70000, 784]. When I tied to split the data into iamge 3 and 9, I didn't know what is 3 and 9 data. After understanding image data and dimension, I could solve this problem.\n",
        "- Understanding narray library. If it is C, many things are impossible. But, I was able to calculate some complicated problem easily thanks to narray such as np.argmin and np.where.\n",
        "\n",
        "\n",
        "## ChatGPT's Answer about Q2\n",
        "\n",
        "Within-cluster distance (compactness): A well-defined cluster should have minimal variance within itself. You can measure how close the data points in each cluster are to the cluster centroid (e.g., using sum of squared distances). The smaller the distance, the more compact the cluster.\n",
        "\n",
        "Between-cluster separation: Clusters should be well-separated from each other. You can check how far the centroids are from each other. Larger distances between centroids suggest better separation between clusters.\n",
        "\n",
        "Silhouette score: This metric calculates how similar a data point is to its own cluster compared to other clusters. Scores range from -1 to 1, where higher values indicate better clustering.\n",
        "\n",
        "Cluster interpretability: If the data is labeled, compare the cluster assignments to the actual labels. The clusters should correspond well to the natural groups in the data (e.g., digits 3 and 9 in the MNIST dataset). For unsupervised data, interpretability could be based on domain knowledge.\n",
        "\n",
        "Elbow method: Plot the sum of squared distances from points to their assigned cluster centers for different numbers of clusters (k). The point at which adding more clusters doesn’t significantly reduce the sum of squared distances indicates the \"optimal\" number of clusters.\n",
        "\n",
        "Visualization (for 2D or 3D data): For lower-dimensional data, visual inspection can be useful. You should be able to see clear separations between clusters, and points should group around centroids.\n",
        "\n",
        "## ChatGPT's Answer about Q3\n",
        "Determining the correct value of \\( k \\) (the number of clusters) for a KMeans algorithm is not always straightforward. Here are several methods and guidelines commonly used to identify the optimal \\( k \\):\n",
        "\n",
        "### 1. **Elbow Method**\n",
        "   - **How it works**: Plot the sum of squared distances (also called inertia or distortion) from each point to its assigned centroid as a function of \\( k \\).\n",
        "   - **What to look for**: As \\( k \\) increases, the within-cluster variance decreases. The \"elbow\" of the plot is the point where the decrease in variance starts to slow down significantly, indicating diminishing returns for adding more clusters. This point is considered a good estimate for the optimal \\( k \\).\n",
        "   - **Why it works**: It identifies a balance between minimizing the number of clusters and maximizing the compactness of the clusters.\n",
        "\n",
        "### 2. **Silhouette Score**\n",
        "   - **How it works**: The silhouette score measures how similar a data point is to its own cluster (cohesion) compared to other clusters (separation). It ranges from -1 to 1.\n",
        "   - **What to look for**: A silhouette score close to 1 indicates well-separated and compact clusters. A score near 0 means the clusters are overlapping, while negative values suggest wrong cluster assignments.\n",
        "   - **Why it works**: It balances within-cluster compactness and separation between clusters, helping find a \\( k \\) that yields well-defined clusters.\n",
        "\n",
        "### 3. **Gap Statistic**\n",
        "   - **How it works**: The gap statistic compares the within-cluster dispersion for different values of \\( k \\) with that expected under a null reference distribution of the data (random distribution). The larger the gap, the better the clustering is compared to random noise.\n",
        "   - **What to look for**: Select \\( k \\) where the gap statistic is largest.\n",
        "   - **Why it works**: It provides a more statistically grounded method to select \\( k \\) and compares actual clusters with clusters that might arise from random data.\n",
        "\n",
        "### 4. **Cross-Validation**\n",
        "   - **How it works**: Use cross-validation techniques, splitting the data into training and validation sets, and evaluate how the KMeans algorithm performs for different values of \\( k \\).\n",
        "   - **What to look for**: Choose the \\( k \\) that performs best on the validation set (e.g., minimizes within-cluster variance or maximizes a specific metric).\n",
        "   - **Why it works**: This method ensures that the chosen \\( k \\) generalizes well to unseen data, avoiding overfitting to noise or random variations.\n",
        "\n",
        "### 5. **BIC/AIC (Bayesian/ Akaike Information Criterion)**\n",
        "   - **How it works**: These criteria add penalties for model complexity (such as increasing \\( k \\)) to the likelihood of the model.\n",
        "   - **What to look for**: Select the value of \\( k \\) that minimizes either the BIC or AIC.\n",
        "   - **Why it works**: It accounts for both the goodness of fit and the simplicity of the model, helping to avoid overfitting by choosing too many clusters.\n",
        "\n",
        "### 6. **Domain Knowledge or Heuristic**\n",
        "   - **How it works**: In some cases, you might have prior knowledge or expectations about the number of clusters. For instance, if you’re clustering types of customers in marketing, there may be a natural grouping based on demographics or buying behavior.\n",
        "   - **What to look for**: Use a combination of domain knowledge and clustering results to select \\( k \\).\n",
        "   - **Why it works**: Domain knowledge helps ensure that the clusters are meaningful in the context of the problem.\n",
        "\n",
        "### 7. **Davies-Bouldin Index**\n",
        "   - **How it works**: This metric evaluates how well clusters are separated by measuring the ratio of within-cluster scatter to between-cluster separation.\n",
        "   - **What to look for**: A lower Davies-Bouldin Index indicates better clustering. You can use this metric to select the optimal \\( k \\).\n",
        "   - **Why it works**: It balances intra-cluster compactness and inter-cluster distance, helping to find well-separated clusters.\n",
        "\n",
        "### 8. **Dunn Index**\n",
        "   - **How it works**: The Dunn Index is the ratio of the minimum inter-cluster distance to the maximum intra-cluster distance.\n",
        "   - **What to look for**: A higher Dunn Index indicates better cluster separation. Use this to select \\( k \\) where the index is maximized.\n",
        "   - **Why it works**: It seeks a balance between tight clusters and large separation between clusters.\n",
        "\n",
        "### Conclusion:\n",
        "There is no single correct answer to choosing \\( k \\) for KMeans, but methods like the **Elbow Method**, **Silhouette Score**, and **Gap Statistic** are commonly used for a more objective assessment. Domain knowledge also plays a crucial role in interpreting the clustering results, ensuring that the clusters are meaningful for the specific context."
      ],
      "metadata": {
        "id": "nAanfQzrBv6B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jy5orwDyBrb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d4-YIsIvToS5"
      }
    }
  ]
}